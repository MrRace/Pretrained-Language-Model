# Pretrained Language Model
目的：收集整理先进的预训练模型

## 目录结构
* [NEZHA](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA) is a pretrained Chinese language model which achieves the state-of-the-art performances on several Chinese NLP tasks.
* [TinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT) is a compressed BERT model which achieves 7.5x smaller and 9.4x faster on inference.
